title(paste(myk,"-NN classification",sep=""))
}
data(wine, package="bootcluster")
library(class)
library(MASS)
# this fn selects uniformly at random n gaussians (whose centers are given by input M)
# and then samples from the selected gaussians one sample for each selected center
generate <- function (M, n=100, Sigma=diag(2)*sqrt(1/4))
{
z <- sample(1:nrow(M), n, replace=TRUE)
t(apply(M[z,], 1, function(mu) mvrnorm(1, mu, Sigma)))
}
# generate 10 means in two dimensions
M0 <- mvrnorm(10, c(1,0), diag(2))
# generate data out of M0 (class 0)
x0 <- generate(M0)
# repeat with M1 (class 1)
M1 <- mvrnorm(10, c(0,1), diag(2))
x1 <- generate(M1)
# Bind them together (by rows)
train <- rbind(x0, x1)
(N <- dim(train)[1])
# generate class labels in {0,1}
t <- c(rep(0,100), rep(1,100))
grid.size <- 100
XLIM <- range(train[,1])
grid.x <- seq(XLIM[1], XLIM[2], len=grid.size)
YLIM <- range(train[,2])
grid.y <- seq(YLIM[1], YLIM[2], len=grid.size)
test <- expand.grid(grid.x,grid.y)
dim(test)
nicecolors <- c('black','red')
visualize.1NN <- function ()
{
par(mfrow=c(1,1))
predicted <- knn(train, test, t, k=1)
# These are the predictions
plot(train, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n")
points(test, col=nicecolors[as.numeric(predicted)], pch=".")
contour(grid.x, grid.y, matrix(as.numeric(predicted),grid.size,grid.size),
levels=c(1,2), add=TRUE, drawlabels=FALSE)
# Add training points, for reference
points(train, col=nicecolors[t+1], pch=16)
title("1-NN classification")
}
visualize.1NN ()
par(mfrow=c(2,3))
for (myk in c(1,3,5,7,10,round(sqrt(N))))
{
predicted <- knn(train, test, t, k=myk)
plot(train, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n")
points(test, col=nicecolors[as.numeric(predicted)], pch=".")
contour(grid.x, grid.y, matrix(as.numeric(predicted),grid.size,grid.size),
levels=c(1,2), add=TRUE, drawlabels=FALSE)
# add training points, for reference
points(train, col=nicecolors[t+1], pch=16)
title(paste(myk,"-NN classification",sep=""))
}
#data(wine, package="bootcluster")
wine <- read.csv("C:/Users/bcoma/Downloads/wine.data", header=FALSE)
dim(wine)
colnames(wine) <- c('Wine.type','Alcohol','Malic.acid','Ash','Alcalinity.of.ash','Magnesium','Total.phenols','Flavanoids','Nonflavanoid.phenols','Proanthocyanins','Color.intensity','Hue','OD280.OD315','Proline')
wine$Wine.type <- factor(wine$Wine.type, labels=c("Cultivar.1","Cultivar.2","Cultivar.3"))
wine.data <- subset (wine, select = -Wine.type)
knn.preds <- rep(NA, nrow(wine.data))
for (i in 1:nrow(wine.data))
{
knn.preds[i] <- knn (wine.data[-i,], wine.data[i,], wine$Wine.type[-i], k = 3)
}
(tab <- table(Truth=wine$Wine.type, Preds=knn.preds))
1 - sum(tab[row(tab)==col(tab)])/sum(tab)
myknn.cv <- knn.cv (wine.data, wine$Wine.type, k = 3)
(tab <- table(Truth=wine$Wine.type, Preds=myknn.cv))
1 - sum(tab[row(tab)==col(tab)])/sum(tab)
set.seed (6046)
N <- nrow(wine)
neighbours <- 1:sqrt(N)
loop.k <- function (mydata, mytargets, myneighbours)
{
errors <- matrix (nrow=length(myneighbours), ncol=2)
colnames(errors) <- c("k","LOOCV error")
for (k in myneighbours)
{
myknn.cv <- knn.cv (mydata, mytargets, k = myneighbours[k])
# fill in number of neighbours and LOOCV error
errors[k, "k"] <- myneighbours[k]
tab <- table(Truth=mytargets, Preds=myknn.cv)
errors[k, "LOOCV error"] <- 1 - sum(tab[row(tab)==col(tab)])/sum(tab)
}
errors
}
par(mfrow=c(1,1))
plot(loop.k (wine.data, wine$Wine.type, neighbours), type="l", xaxt = "n")
axis(1, neighbours)
plot(loop.k (scale(wine.data), wine$Wine.type, neighbours), type="l", xaxt = "n")
axis(1, neighbours)
lda.model <- lda (Wine.type ~ ., data = wine)
loadings <- as.matrix(wine.data) %*% as.matrix(lda.model$scaling)
set.seed (6046)
plot(loop.k (loadings, wine$Wine.type, neighbours), type="l", xaxt = "n")
axis(1, neighbours)
library (e1071)
data(HouseVotes84, package="mlbench")
colnames(HouseVotes84) <-
c("Class","handicapped.infants","water.project.sharing","budget.resolution",
"physician.fee.freeze","el.salvador.aid","religious.groups.in.schools",
"anti.satellite.ban", "aid.to.nicaraguan.contras","mx.missile",
"immigration","synfuels.cutback","education.spending","superfund","crime",
"duty.free.exports","export.South.Africa")
summary(HouseVotes84)
set.seed(1111)
N <- nrow(HouseVotes84)
learn <- sample(1:N, round(2*N/3))
nlearn <- length(learn)
ntest <- N - nlearn
(model <- naiveBayes(Class ~ ., data = HouseVotes84[learn,]))
predict(model, HouseVotes84[1:10,-1])
predict(model, HouseVotes84[1:10,-1], type = "raw")
pred <- predict(model, HouseVotes84[learn,-1])
(tab <- table(Truth=HouseVotes84[learn,]$Class, Preds=pred))  # confusion matrix
1 - sum(tab[row(tab)==col(tab)])/sum(tab)                     # training error
pred <- predict(model, newdata=HouseVotes84[-learn,-1])
(tab <- table(Truth=HouseVotes84[-learn,]$Class, Preds=pred) )   # confusion matrix
1 - sum(tab[row(tab)==col(tab)])/sum(tab)                        # overall test error
model2 <- naiveBayes(Class ~ ., data = HouseVotes84[learn,], laplace = 1)
pred <- predict(model2, newdata=HouseVotes84[-learn,-1])
(tab <- table(Truth=HouseVotes84[-learn,]$Class, Preds=pred) )   # confusion matrix
1 - sum(tab[row(tab)==col(tab)])/sum(tab)                        # overall test error
?naiveBayes
?rda
p
set.seed(1234)
N <- 500
x <- rnorm(n=N, mean=3, sd=2)     # generate the x_n data (note x is a vector)
beta_1 <- 0.6 ; beta_0 <- -1.5    # this is the ground truth, which is unknown
p <- 1/(1+exp( -(beta_1*x + beta_0) ))  # generate the p_x (note p is a vector)
t <- rbinom (n=N, size=1, prob=p)    # generate the targets (classes) t_n according to the p_n
t <- as.factor(t)                  # note t is a vector
plot(x, as.numeric(t)-1, xlab="x_n", ylab="t_n")
glm.res <- glm (t~x, family = binomial(link=logit)) # actually link=logit is the default for binomial!
summary(glm.res)
coef(glm.res)
glm.res$coefficients["x"]
glm.res$coefficients["(Intercept)"]
exp(glm.res$coefficients["x"])
M <- max(x)
m <- min(x)
abscissae <- m + (-10:210)*(M-m)/200
preds <- predict(glm.res, data.frame(x=abscissae), type="response")
plot(p~x,ylim=c(0,1)) # plot previous data
lines(abscissae, preds, col="blue") # add our model, quite good!
p
t
p
p
t
p
p
t
p
N <- 3#500
x <- rnorm(n=N, mean=3, sd=2)     # generate the x_n data (note x is a vector)
beta_1 <- 0.6 ; beta_0 <- -1.5    # this is the ground truth, which is unknown
p <- 1/(1+exp( -(beta_1*x + beta_0) ))  # generate the p_x (note p is a vector)
t <- rbinom (n=N, size=1, prob=p)    # generate the targets (classes) t_n according to the p_n
t <- as.factor(t)                  # note t is a vector
plot(x, as.numeric(t)-1, xlab="x_n", ylab="t_n")
p
N <- 500
x <- rnorm(n=N, mean=3, sd=2)     # generate the x_n data (note x is a vector)
beta_1 <- 0.6 ; beta_0 <- -1.5    # this is the ground truth, which is unknown
p <- 1/(1+exp( -(beta_1*x + beta_0) ))  # generate the p_x (note p is a vector)
t <- rbinom (n=N, size=1, prob=p)    # generate the targets (classes) t_n according to the p_n
t <- as.factor(t)                  # note t is a vector
N <- 500
x <- rnorm(n=N, mean=3, sd=2)     # generate the x_n data (note x is a vector)
beta_1 <- 0.6 ; beta_0 <- -1.5    # this is the ground truth, which is unknown
p <- 1/(1+exp( -(beta_1*x + beta_0) ))  # generate the p_x (note p is a vector)
t <- rbinom (n=N, size=1, prob=p)    # generate the targets (classes) t_n according to the p_n
t <- as.factor(t)                  # note t is a vector
p
t
plot(x, as.numeric(t)-1, xlab="x_n", ylab="t_n")
p
t
p
?rbinom
?rbinom
x
p
N <- 500
x <- rnorm(n=N, mean=3, sd=2)     # generate the x_n data (note x is a vector)
beta_1 <- 0.6 ; beta_0 <- -1.5    # this is the ground truth, which is unknown
p <- 1/(1+exp( -(beta_1*x + beta_0) ))  # generate the p_x (note p is a vector)
t <- rbinom (n=N, size=1, prob=p)    # generate the targets (classes) t_n according to the p_n
t <- as.factor(t)                  # note t is a vector
set.seed(1234)
N <- 500
x <- rnorm(n=N, mean=3, sd=2)     # generate the x_n data (note x is a vector)
beta_1 <- 0.6 ; beta_0 <- -1.5    # this is the ground truth, which is unknown
p <- 1/(1+exp( -(beta_1*x + beta_0) ))  # generate the p_x (note p is a vector)
t <- rbinom (n=N, size=1, prob=p)    # generate the targets (classes) t_n according to the p_n
t <- as.factor(t)                  # note t is a vector
plot(x, as.numeric(t)-1, xlab="x_n", ylab="t_n")
p
x
?rnorm
p
x
p
x
p
x
p
x
p
x
p
x
?rpois
l
set.seed(1234)
N <- 500
x <- runif(n=N,0.1,12)            # generate the x_n (note x is a vector)
beta_1 <- 0.35 ; beta_0 <- -1     # this is the ground truth, which is unknown
l <- exp(beta_1*x + beta_0)       # generate the lambdas (note l is a vector)
t <- rpois(n=N, lambda = l)       # generate the targets t_n according to parameter l_n for each x_n
plot(x, t, xlab="Distance to workplace (km)", ylab="Time wasted (h/week)")
mydata <- data.frame(h.week=t, dist=x)
glm.res <- glm(h.week ~ dist, family = poisson(link="log"), data = mydata)
summary(glm.res)
new.d <- seq(0,30,length.out=100)
fv <- predict (glm.res, data.frame(dist=new.d), se=TRUE)
plot (x, t, xlab="Distance to workplace (km)", ylab="Time wasted (h/week)")
lines(new.d,exp(fv$fit), col='red')
plot (x, t, xlab="Distance to workplace (km)", ylab="Time wasted (h/week)")
lines (new.d,exp(fv$fit), col='red')
lines (new.d,exp(fv$fit+1.967*fv$se.fit), col='red',lty=2)
lines (new.d,exp(fv$fit-1.967*fv$se.fit), col='red',lty=2)
l
t
l
t
rpois(n=N, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
library(MASS)
library(nnet)
set.seed(3)
par(mfrow=c(1,1))
p <- 2
N <- 200
x <- matrix(rnorm(N*p),ncol=p)
y <- as.numeric((x[,1]^2+x[,2]^2) > 1.4)
mydata <- data.frame(x=x,y=y)
plot(x, col=c('black','green')[y+1], pch=19, asp=1)
set.seed(3)
nn1 <- nnet(y~x.1+x.2, data=mydata, entropy=T, size=3, decay=0, maxit=2000, trace=F)
yhat <- as.numeric(predict(nn1,type='class'))
par(mfrow=c(1,2))
plot(x,pch=19,col=c('black','green')[y+1],main='actual labels',asp=1)
plot(x,col=c('black','green')[(yhat>0.5)+1],pch=19,main='predicted labels',asp=1)
table(actual=y,predicted=predict(nn1,type='class'))
set.seed(9)
nn1 <- nnet(y~x.1+x.2, data=mydata, entropy=T, size=3, decay=0, maxit=2000, trace=F)
yhat <- as.numeric(predict(nn1,type='class'))
par(mfrow=c(1,2))
plot(x,pch=19,col=c('black','green')[y+1],main='actual labels',asp=1)
plot(x,col=c('black','green')[(yhat>0.5)+1],pch=19,main='predicted labels',asp=1)
table(actual=y,predicted=predict(nn1,type='class'))
par(mfrow=c(2,2))
for (i in 1:4)
{
set.seed(3)
nn1 <- nnet(y~x.1+x.2, data=mydata, entropy=T, size=i, decay=0, maxit=2000, trace=F)
yhat <- as.numeric(predict(nn1,type='class'))
plot(x,pch=20,col=c('black','green')[yhat+1])
title(main=paste('nnet with',i,'hidden unit(s)'))
}
set.seed(3)
nn1 <- nnet(y~x.1+x.2, data=mydata, entropy=T, size=3, decay=0, maxit=2000, trace=F)
# create a grid of values
x1grid <- seq(-3,3,l=200)
x2grid <- seq(-3,3,l=220)
xg <- expand.grid(x1grid,x2grid)
xg <- as.matrix(cbind(1,xg))
## input them to the hidden units, and get their outputs
h1 <- xg%*%matrix(coef(nn1)[1:3],ncol=1)
h2 <- xg%*%matrix(coef(nn1)[4:6],ncol=1)
h3 <- xg%*%matrix(coef(nn1)[7:9],ncol=1)
## this is the logistic function, used by nnet() for the hidden neurons, and
## for the output neurons in two-class classification problems
logistic <- function(x) {1/(1+exp(-x))}
## the hidden units compute the logistic function, so we cut the output value at 0.5; we get a decision line
par(mfrow=c(2,2))
contour(x1grid, x2grid, matrix(h1, 200, 220), levels=0.5)
contour(x1grid, x2grid, matrix(h2, 200, 220), levels=0.5, add=T)
contour(x1grid, x2grid, matrix(h3, 200, 220), levels=0.5, add=T)
title(main='net input = 0.5\n in the hidden units')
z <- coef(nn1)[10] + coef(nn1)[11]*logistic(h1) + coef(nn1)[12]*logistic(h2) + coef(nn1)[13]*logistic(h3)
contour(x1grid,x2grid,matrix(z,200,220))
title('hidden outputs = logistic of the net inputs\n and their weighted sum')
contour(x1grid,x2grid,matrix(logistic(z),200,220),levels=0.5)
title('logistic of the previous sum')
contour(x1grid,x2grid,matrix(logistic(z),200,220),levels=0.5)
points(x,pch=20,col=c('black','green')[y+1])
title('same with training data points')
?nnet
?nnet
?nnet
?nnet
?nnet0
Admis <- read.csv("Admissions.csv")
head(Admis)
Admis$admit <- factor(Admis$admit, labels=c("No","Yes"))
summary(Admis)
dim(Admis)
set.seed(1234)
N <- nrow(Admis)
learn <- sample(1:N, round(2*N/3))  # random indices for the learning set
nlearn <- length(learn)
ntest <- N - nlearn
library(MASS)
library(nnet)
Admis$gpa <- scale(Admis$gpa)
Admis$gre <- scale(Admis$gre)
Admis$rank <- scale(Admis$rank)
model.nnet0 <- multinom(admit ~., data = Admis, subset=learn, maxit=200)
errors <- function (model)
{
options(digits=4)
p1 <- as.factor(predict (model, type="class"))
t1 <- table(p1,Admis$admit[learn])
cat ("Train = ", 100*(1-sum(diag(t1))/nlearn),"%\n")
p2 <- as.factor(predict (model, newdata=Admis[-learn,], type="class"))
t2 <- table(p2,Admis$admit[-learn])
cat ("Test =  ", 100*(1-sum(diag(t2))/ntest),"%\n")
}
errors (model.nnet0)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=2, maxit=200, decay=0)
model.nnet
model.nnet$value
model.nnet$fitted.values[1:10,]
model.nnet$residuals[1:10,]
model.nnet$wts
summary(model.nnet)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=2, maxit=200, decay=0.5)
summary(model.nnet)
errors (model.nnet)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=30, maxit=500)
errors (model.nnet)
library(caret)
(sizes <- 2*seq(1,10,by=1))
trc <- trainControl (method="repeatedcv", number=10, repeats=10)
model.10x10CV <- train (admit ~., data = Admis, subset=learn,
method='nnet', maxit = 500, trace = FALSE,
tuneGrid = expand.grid(.size=sizes,.decay=0), trControl=trc)
model.10x10CV$results
model.10x10CV$bestTune
(decays <- 10^seq(-2, 0, by=0.2))
model.10x10CV <- train (admit ~., data = Admis, subset=learn, method='nnet',
maxit = 500, trace = FALSE,
tuneGrid = expand.grid(.size=20,.decay=decays), trControl=trc)
model.10x10CV$results
model.10x10CV$bestTune
p2 <- as.factor(predict (model.10x10CV, newdata=Admis[-learn,], type="raw"))
t2 <- table(pred=p2,truth=Admis$admit[-learn])
(error_rate.test <- 100*(1-sum(diag(t2))/ntest))
t2
?nnet
?multinom
model.nnet0 <- nnet(admit ~., data = Admis, subset=learn, maxit=200)
model.nnet0 <- nnet(admit ~., data = Admis, subset=learn, maxit=200)
model.nnet0 <- nnet(admit ~., data = Admis, subset=learn, maxit=200)
Admis <- read.csv("Admissions.csv")
head(Admis)
Admis$admit <- factor(Admis$admit, labels=c("No","Yes"))
summary(Admis)
dim(Admis)
set.seed(1234)
N <- nrow(Admis)
learn <- sample(1:N, round(2*N/3))  # random indices for the learning set
nlearn <- length(learn)
ntest <- N - nlearn
library(MASS)
library(nnet)
Admis$gpa <- scale(Admis$gpa)
Admis$gre <- scale(Admis$gre)
Admis$rank <- scale(Admis$rank)
model.nnet0 <- multinom(admit ~., data = Admis, subset=learn, maxit=200)
errors <- function (model)
{
options(digits=4)
p1 <- as.factor(predict (model, type="class"))
t1 <- table(p1,Admis$admit[learn])
cat ("Train = ", 100*(1-sum(diag(t1))/nlearn),"%\n")
p2 <- as.factor(predict (model, newdata=Admis[-learn,], type="class"))
t2 <- table(p2,Admis$admit[-learn])
cat ("Test =  ", 100*(1-sum(diag(t2))/ntest),"%\n")
}
errors (model.nnet0)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=2, maxit=200, decay=0)
model.nnet
model.nnet$value
model.nnet$fitted.values[1:10,]
model.nnet$residuals[1:10,]
model.nnet$wts
summary(model.nnet)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=2, maxit=200, decay=0.5)
summary(model.nnet)
errors (model.nnet)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=30, maxit=500)
errors (model.nnet)
library(caret)
(sizes <- 2*seq(1,10,by=1))
trc <- trainControl (method="repeatedcv", number=10, repeats=10)
model.10x10CV <- train (admit ~., data = Admis, subset=learn,
method='nnet', maxit = 500, trace = FALSE,
tuneGrid = expand.grid(.size=sizes,.decay=0), trControl=trc)
model.10x10CV$results
model.10x10CV$bestTune
(decays <- 10^seq(-2, 0, by=0.2))
model.10x10CV <- train (admit ~., data = Admis, subset=learn, method='nnet',
maxit = 500, trace = FALSE,
tuneGrid = expand.grid(.size=20,.decay=decays), trControl=trc)
model.10x10CV$results
model.10x10CV$bestTune
p2 <- as.factor(predict (model.10x10CV, newdata=Admis[-learn,], type="raw"))
t2 <- table(pred=p2,truth=Admis$admit[-learn])
(error_rate.test <- 100*(1-sum(diag(t2))/ntest))
t2
?nnet
library(readxl)
#install.packages("ltm")
#install.packages("polycor")
#install.packages("glmmTMB")
library(polycor)
library("glmmTMB")
library("DescTools")
#cor <- correlation(allInfo)
#summary(cor)
allInfo <- read_excel('../output/allExcels_negatiu.xlsx')
setwd("C:/Users/bcoma/Documents/GitHub/Tesis_UB/scripts")
library(readxl)
#install.packages("ltm")
#install.packages("polycor")
#install.packages("glmmTMB")
library(polycor)
library("glmmTMB")
library("DescTools")
#cor <- correlation(allInfo)
#summary(cor)
allInfo <- read_excel('../output/allExcels_negatiu.xlsx')
allInfoLog <- allInfo[,]
allInfoLog['Budget_Previous_Year'] <- log2(allInfoLog['Budget_Previous_Year'])
allInfoLog['Donor_Aid_Budget'] <- log2(allInfoLog['Donor_Aid_Budget'])
allInfoLog['GDP'] <- log2(allInfoLog['GDP'])
allInfoLog['Public_Grant'] <- log2(allInfoLog['Public_Grant'])
allInfoLog[allInfoLog<0] <- 0
table(allInfoLog$Visitado)
modelbin1 <- glm(Visitado~ONU+GDP+Public_Grant+Budget_Previous_Year+Donor_Aid_Budget+LatinAmerica+Africa+Colony+Delegation,data=allInfoLog,family = "binomial")
summary(modelbin1)
PseudoR2(modelbin1,"McFadden")
modelbin3 <- glm(Visitado~Public_Grant+Donor_Aid_Budget+Colony,data=allInfoLog,family = "binomial")
summary(modelbin3)
PseudoR2(modelbin3,"McFadden")
modelbin4 <- glm(Visitado~ONU+GDP+LatinAmerica+Africa+Colony,data=allInfoLog,family = "binomial")
summary(modelbin4)
PseudoR2(modelbin4,"McFadden")
modelbin5 <- glm(Visitado~Budget_Previous_Year+Delegation+Colony,data=allInfoLog,family = "binomial")
summary(modelbin5)
PseudoR2(modelbin5,"McFadden")
