N <- 500
x <- rnorm(n=N, mean=3, sd=2)     # generate the x_n data (note x is a vector)
beta_1 <- 0.6 ; beta_0 <- -1.5    # this is the ground truth, which is unknown
p <- 1/(1+exp( -(beta_1*x + beta_0) ))  # generate the p_x (note p is a vector)
t <- rbinom (n=N, size=1, prob=p)    # generate the targets (classes) t_n according to the p_n
t <- as.factor(t)                  # note t is a vector
p
t
plot(x, as.numeric(t)-1, xlab="x_n", ylab="t_n")
p
t
p
?rbinom
?rbinom
x
p
N <- 500
x <- rnorm(n=N, mean=3, sd=2)     # generate the x_n data (note x is a vector)
beta_1 <- 0.6 ; beta_0 <- -1.5    # this is the ground truth, which is unknown
p <- 1/(1+exp( -(beta_1*x + beta_0) ))  # generate the p_x (note p is a vector)
t <- rbinom (n=N, size=1, prob=p)    # generate the targets (classes) t_n according to the p_n
t <- as.factor(t)                  # note t is a vector
set.seed(1234)
N <- 500
x <- rnorm(n=N, mean=3, sd=2)     # generate the x_n data (note x is a vector)
beta_1 <- 0.6 ; beta_0 <- -1.5    # this is the ground truth, which is unknown
p <- 1/(1+exp( -(beta_1*x + beta_0) ))  # generate the p_x (note p is a vector)
t <- rbinom (n=N, size=1, prob=p)    # generate the targets (classes) t_n according to the p_n
t <- as.factor(t)                  # note t is a vector
plot(x, as.numeric(t)-1, xlab="x_n", ylab="t_n")
p
x
?rnorm
p
x
p
x
p
x
p
x
p
x
p
x
?rpois
l
set.seed(1234)
N <- 500
x <- runif(n=N,0.1,12)            # generate the x_n (note x is a vector)
beta_1 <- 0.35 ; beta_0 <- -1     # this is the ground truth, which is unknown
l <- exp(beta_1*x + beta_0)       # generate the lambdas (note l is a vector)
t <- rpois(n=N, lambda = l)       # generate the targets t_n according to parameter l_n for each x_n
plot(x, t, xlab="Distance to workplace (km)", ylab="Time wasted (h/week)")
mydata <- data.frame(h.week=t, dist=x)
glm.res <- glm(h.week ~ dist, family = poisson(link="log"), data = mydata)
summary(glm.res)
new.d <- seq(0,30,length.out=100)
fv <- predict (glm.res, data.frame(dist=new.d), se=TRUE)
plot (x, t, xlab="Distance to workplace (km)", ylab="Time wasted (h/week)")
lines(new.d,exp(fv$fit), col='red')
plot (x, t, xlab="Distance to workplace (km)", ylab="Time wasted (h/week)")
lines (new.d,exp(fv$fit), col='red')
lines (new.d,exp(fv$fit+1.967*fv$se.fit), col='red',lty=2)
lines (new.d,exp(fv$fit-1.967*fv$se.fit), col='red',lty=2)
l
t
l
t
rpois(n=N, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 2)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 8)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
rpois(1, 100)
library(MASS)
library(nnet)
set.seed(3)
par(mfrow=c(1,1))
p <- 2
N <- 200
x <- matrix(rnorm(N*p),ncol=p)
y <- as.numeric((x[,1]^2+x[,2]^2) > 1.4)
mydata <- data.frame(x=x,y=y)
plot(x, col=c('black','green')[y+1], pch=19, asp=1)
set.seed(3)
nn1 <- nnet(y~x.1+x.2, data=mydata, entropy=T, size=3, decay=0, maxit=2000, trace=F)
yhat <- as.numeric(predict(nn1,type='class'))
par(mfrow=c(1,2))
plot(x,pch=19,col=c('black','green')[y+1],main='actual labels',asp=1)
plot(x,col=c('black','green')[(yhat>0.5)+1],pch=19,main='predicted labels',asp=1)
table(actual=y,predicted=predict(nn1,type='class'))
set.seed(9)
nn1 <- nnet(y~x.1+x.2, data=mydata, entropy=T, size=3, decay=0, maxit=2000, trace=F)
yhat <- as.numeric(predict(nn1,type='class'))
par(mfrow=c(1,2))
plot(x,pch=19,col=c('black','green')[y+1],main='actual labels',asp=1)
plot(x,col=c('black','green')[(yhat>0.5)+1],pch=19,main='predicted labels',asp=1)
table(actual=y,predicted=predict(nn1,type='class'))
par(mfrow=c(2,2))
for (i in 1:4)
{
set.seed(3)
nn1 <- nnet(y~x.1+x.2, data=mydata, entropy=T, size=i, decay=0, maxit=2000, trace=F)
yhat <- as.numeric(predict(nn1,type='class'))
plot(x,pch=20,col=c('black','green')[yhat+1])
title(main=paste('nnet with',i,'hidden unit(s)'))
}
set.seed(3)
nn1 <- nnet(y~x.1+x.2, data=mydata, entropy=T, size=3, decay=0, maxit=2000, trace=F)
# create a grid of values
x1grid <- seq(-3,3,l=200)
x2grid <- seq(-3,3,l=220)
xg <- expand.grid(x1grid,x2grid)
xg <- as.matrix(cbind(1,xg))
## input them to the hidden units, and get their outputs
h1 <- xg%*%matrix(coef(nn1)[1:3],ncol=1)
h2 <- xg%*%matrix(coef(nn1)[4:6],ncol=1)
h3 <- xg%*%matrix(coef(nn1)[7:9],ncol=1)
## this is the logistic function, used by nnet() for the hidden neurons, and
## for the output neurons in two-class classification problems
logistic <- function(x) {1/(1+exp(-x))}
## the hidden units compute the logistic function, so we cut the output value at 0.5; we get a decision line
par(mfrow=c(2,2))
contour(x1grid, x2grid, matrix(h1, 200, 220), levels=0.5)
contour(x1grid, x2grid, matrix(h2, 200, 220), levels=0.5, add=T)
contour(x1grid, x2grid, matrix(h3, 200, 220), levels=0.5, add=T)
title(main='net input = 0.5\n in the hidden units')
z <- coef(nn1)[10] + coef(nn1)[11]*logistic(h1) + coef(nn1)[12]*logistic(h2) + coef(nn1)[13]*logistic(h3)
contour(x1grid,x2grid,matrix(z,200,220))
title('hidden outputs = logistic of the net inputs\n and their weighted sum')
contour(x1grid,x2grid,matrix(logistic(z),200,220),levels=0.5)
title('logistic of the previous sum')
contour(x1grid,x2grid,matrix(logistic(z),200,220),levels=0.5)
points(x,pch=20,col=c('black','green')[y+1])
title('same with training data points')
?nnet
?nnet
?nnet
?nnet
?nnet0
Admis <- read.csv("Admissions.csv")
head(Admis)
Admis$admit <- factor(Admis$admit, labels=c("No","Yes"))
summary(Admis)
dim(Admis)
set.seed(1234)
N <- nrow(Admis)
learn <- sample(1:N, round(2*N/3))  # random indices for the learning set
nlearn <- length(learn)
ntest <- N - nlearn
library(MASS)
library(nnet)
Admis$gpa <- scale(Admis$gpa)
Admis$gre <- scale(Admis$gre)
Admis$rank <- scale(Admis$rank)
model.nnet0 <- multinom(admit ~., data = Admis, subset=learn, maxit=200)
errors <- function (model)
{
options(digits=4)
p1 <- as.factor(predict (model, type="class"))
t1 <- table(p1,Admis$admit[learn])
cat ("Train = ", 100*(1-sum(diag(t1))/nlearn),"%\n")
p2 <- as.factor(predict (model, newdata=Admis[-learn,], type="class"))
t2 <- table(p2,Admis$admit[-learn])
cat ("Test =  ", 100*(1-sum(diag(t2))/ntest),"%\n")
}
errors (model.nnet0)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=2, maxit=200, decay=0)
model.nnet
model.nnet$value
model.nnet$fitted.values[1:10,]
model.nnet$residuals[1:10,]
model.nnet$wts
summary(model.nnet)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=2, maxit=200, decay=0.5)
summary(model.nnet)
errors (model.nnet)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=30, maxit=500)
errors (model.nnet)
library(caret)
(sizes <- 2*seq(1,10,by=1))
trc <- trainControl (method="repeatedcv", number=10, repeats=10)
model.10x10CV <- train (admit ~., data = Admis, subset=learn,
method='nnet', maxit = 500, trace = FALSE,
tuneGrid = expand.grid(.size=sizes,.decay=0), trControl=trc)
model.10x10CV$results
model.10x10CV$bestTune
(decays <- 10^seq(-2, 0, by=0.2))
model.10x10CV <- train (admit ~., data = Admis, subset=learn, method='nnet',
maxit = 500, trace = FALSE,
tuneGrid = expand.grid(.size=20,.decay=decays), trControl=trc)
model.10x10CV$results
model.10x10CV$bestTune
p2 <- as.factor(predict (model.10x10CV, newdata=Admis[-learn,], type="raw"))
t2 <- table(pred=p2,truth=Admis$admit[-learn])
(error_rate.test <- 100*(1-sum(diag(t2))/ntest))
t2
?nnet
?multinom
model.nnet0 <- nnet(admit ~., data = Admis, subset=learn, maxit=200)
model.nnet0 <- nnet(admit ~., data = Admis, subset=learn, maxit=200)
model.nnet0 <- nnet(admit ~., data = Admis, subset=learn, maxit=200)
Admis <- read.csv("Admissions.csv")
head(Admis)
Admis$admit <- factor(Admis$admit, labels=c("No","Yes"))
summary(Admis)
dim(Admis)
set.seed(1234)
N <- nrow(Admis)
learn <- sample(1:N, round(2*N/3))  # random indices for the learning set
nlearn <- length(learn)
ntest <- N - nlearn
library(MASS)
library(nnet)
Admis$gpa <- scale(Admis$gpa)
Admis$gre <- scale(Admis$gre)
Admis$rank <- scale(Admis$rank)
model.nnet0 <- multinom(admit ~., data = Admis, subset=learn, maxit=200)
errors <- function (model)
{
options(digits=4)
p1 <- as.factor(predict (model, type="class"))
t1 <- table(p1,Admis$admit[learn])
cat ("Train = ", 100*(1-sum(diag(t1))/nlearn),"%\n")
p2 <- as.factor(predict (model, newdata=Admis[-learn,], type="class"))
t2 <- table(p2,Admis$admit[-learn])
cat ("Test =  ", 100*(1-sum(diag(t2))/ntest),"%\n")
}
errors (model.nnet0)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=2, maxit=200, decay=0)
model.nnet
model.nnet$value
model.nnet$fitted.values[1:10,]
model.nnet$residuals[1:10,]
model.nnet$wts
summary(model.nnet)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=2, maxit=200, decay=0.5)
summary(model.nnet)
errors (model.nnet)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=30, maxit=500)
errors (model.nnet)
library(caret)
(sizes <- 2*seq(1,10,by=1))
trc <- trainControl (method="repeatedcv", number=10, repeats=10)
model.10x10CV <- train (admit ~., data = Admis, subset=learn,
method='nnet', maxit = 500, trace = FALSE,
tuneGrid = expand.grid(.size=sizes,.decay=0), trControl=trc)
model.10x10CV$results
model.10x10CV$bestTune
(decays <- 10^seq(-2, 0, by=0.2))
model.10x10CV <- train (admit ~., data = Admis, subset=learn, method='nnet',
maxit = 500, trace = FALSE,
tuneGrid = expand.grid(.size=20,.decay=decays), trControl=trc)
model.10x10CV$results
model.10x10CV$bestTune
p2 <- as.factor(predict (model.10x10CV, newdata=Admis[-learn,], type="raw"))
t2 <- table(pred=p2,truth=Admis$admit[-learn])
(error_rate.test <- 100*(1-sum(diag(t2))/ntest))
t2
?nnet
library(kernlab)
data(spam)
# log-transform count columns
spam[,55:57] <- as.matrix(log10(spam[,55:57]+1))
spam2 <- spam[spam$george==0,]
spam2 <- spam2[spam2$num650==0,]
spam2 <- spam2[spam2$hp==0,]
spam2 <- spam2[spam2$hpl==0,]
george.vars <- 25:28
spam2 <- spam2[,-george.vars]
moneys.vars <- c(16,17,20,24)
spam3 <- data.frame( spam2[,-moneys.vars], spam2[,16]+spam2[,17]+spam2[,20]+spam2[,24])
colnames(spam3)[51] <- "about.money"
dim(spam3)
summary(spam3)
set.seed(4321)
N <- nrow(spam3)
learn <- sample(1:N, round(2/3*N))
nlearn <- length(learn)
library(tree)
model.tree <- tree(type ~ ., data=spam3[learn,])
summary(model.tree)
model.tree
plot (model.tree)
text (model.tree,pretty=0)
pred.tree <- predict (model.tree, spam3[-learn,], type="class")
(ct <- table(Truth=spam3[-learn,]$type, Pred=pred.tree))
table(spam3[-learn,]$type)/sum(table(spam3[-learn,]$type))*100
# percent by class
prop.table(ct, 1)
sum(diag(ct))/sum(ct)
100*(1-sum(diag(ct))/sum(ct))
harm <- function (a,b) { 2/(1/a+1/b) }   # harmonic mean
(F1 <- harm (prop.table(ct,1)[1,1], prop.table(ct,1)[2,2]))  # harmonic mean of precision and recall
library(randomForest)
model.rf1 <- randomForest (type ~ ., data=spam3[learn,], ntree=100, proximity=FALSE)
model.rf1
pred.rf1 <- predict (model.rf1, spam3[-learn,], type="class")   # predict using trained model
(ct <- table(Truth=spam3[-learn,]$type, Pred=pred.rf1))         # confusion matrix
(prop.table(ct, 1))                                             # error rate per class
round(100*(1-sum(diag(ct))/sum(ct)),2)
(F1 <- harm (prop.table(ct,1)[1,1], prop.table(ct,1)[2,2]))
(N.nonspam <- table(spam3[learn,]$type)["nonspam"])
(N.spam <- table(spam3[learn,]$type)["spam"])
model.rf2 <- randomForest(type ~ ., data=spam3[learn,], ntree=100, proximity=FALSE,
sampsize=c(nonspam=800, spam=500), strata=spam3[learn,]$type)
model.rf2
pred.rf2 <- predict (model.rf2, spam3[-learn,], type="class")
(ct <- table(Truth=spam3[-learn,]$type, Pred=pred.rf2))         # confusion matrix on test set
(prop.table(ct, 1))                                             # test error rates per class
(round(100*(1-sum(diag(ct))/sum(ct)),2))                        # test error rate
(F1 <- harm (prop.table(ct,1)[1,1], prop.table(ct,1)[2,2]))     # F1 measure on test set
(ntrees <- round(2^seq(1,10)))
rf.results <- matrix (rep(0,2*length(ntrees)), nrow=length(ntrees))
colnames (rf.results) <- c("ntrees", "OOB")
rf.results[,"ntrees"] <- ntrees
rf.results[,"OOB"] <- 0
ii <- 1
for (nt in ntrees)
{
print(nt)
# build forest
model.rf <- randomForest(type ~ ., data=spam3[learn,], ntree=nt, proximity=FALSE,
sampsize=c(nonspam=800, spam=500), strata=spam3[learn,]$type)
# get the OOB and store it appropriately
rf.results[ii, "OOB"] <- model.rf$err.rate[nt,1]
ii <- ii+1
}
rf.results
lowest.OOB.error <- as.integer(which.min(rf.results[,"OOB"]))
(ntrees.best <- rf.results[lowest.OOB.error,"ntrees"])
model.rf3 <- randomForest(type ~ ., data=spam3[learn,], ntree=ntrees.best, proximity=FALSE,
sampsize=c(nonspam=800, spam=500), strata=spam3[learn,]$type)
pred.rf3 <- predict (model.rf3, spam3[-learn,], type="class")
ct <- table(Truth=spam3[-learn,]$type, Pred=pred.rf3)
(prop.table(ct, 1))                             # test set error by class
(round(100*(1-sum(diag(ct))/sum(ct)),2))        # test set error
(F1 <- harm (prop.table(ct,1)[1,1], prop.table(ct,1)[2,2]))  # F1 measure
print(model.rf3)
summary(model.rf3)
model.rf3$oob.times
model.rf3$confusion
varUsed(model.rf3)
importance(model.rf3)
varImpPlot(model.rf3)
plot(model.rf3)
legend("topright", legend=c("OOB", "nonspam OOB", "spam OOB"), pch=c(1,1), col=c("black","red","green"))
# 1. build forest on the whole spam3 data
rf <- randomForest (type ~ ., data = spam3, ntree=1000, proximity=TRUE, importance=FALSE)
# 2. compute MDS based on pair-wise proximities from random forest and plot
rf.plot <- MDSplot(rf, spam3$type, pch=as.numeric(spam3$type))
# 3. add nice stuff to plot
library(RColorBrewer)
title("Data visualization based on MDS projection of the Random Forest")
legend("topright", legend=levels(spam3$type),
fill=brewer.pal(length(levels(spam3$type)), "Set1"))
# 4. need to identify some points? here, we include labels for first 100 emails..
text(rf.plot$points[1:100,], labels=attr(rf.plot$points,"dimnames")[[1]][1:10], cex=0.4)
library(kernlab)
data(spam)
# log-transform count columns
spam[,55:57] <- as.matrix(log10(spam[,55:57]+1))
spam2 <- spam[spam$george==0,]
spam2 <- spam2[spam2$num650==0,]
spam2 <- spam2[spam2$hp==0,]
spam2 <- spam2[spam2$hpl==0,]
george.vars <- 25:28
spam2 <- spam2[,-george.vars]
moneys.vars <- c(16,17,20,24)
spam3 <- data.frame( spam2[,-moneys.vars], spam2[,16]+spam2[,17]+spam2[,20]+spam2[,24])
colnames(spam3)[51] <- "about.money"
dim(spam3)
summary(spam3)
set.seed(4321)
N <- nrow(spam3)
learn <- sample(1:N, round(2/3*N))
nlearn <- length(learn)
library(tree)
model.tree <- tree(type ~ ., data=spam3[learn,])
summary(model.tree)
model.tree
plot (model.tree)
text (model.tree,pretty=0)
pred.tree <- predict (model.tree, spam3[-learn,], type="class")
(ct <- table(Truth=spam3[-learn,]$type, Pred=pred.tree))
table(spam3[-learn,]$type)/sum(table(spam3[-learn,]$type))*100
# percent by class
prop.table(ct, 1)
sum(diag(ct))/sum(ct)
100*(1-sum(diag(ct))/sum(ct))
harm <- function (a,b) { 2/(1/a+1/b) }   # harmonic mean
(F1 <- harm (prop.table(ct,1)[1,1], prop.table(ct,1)[2,2]))  # harmonic mean of precision and recall
library(randomForest)
model.rf1 <- randomForest (type ~ ., data=spam3[learn,], ntree=100, proximity=FALSE)
model.rf1
pred.rf1 <- predict (model.rf1, spam3[-learn,], type="class")   # predict using trained model
(ct <- table(Truth=spam3[-learn,]$type, Pred=pred.rf1))         # confusion matrix
(prop.table(ct, 1))                                             # error rate per class
round(100*(1-sum(diag(ct))/sum(ct)),2)
(F1 <- harm (prop.table(ct,1)[1,1], prop.table(ct,1)[2,2]))
(N.nonspam <- table(spam3[learn,]$type)["nonspam"])
(N.spam <- table(spam3[learn,]$type)["spam"])
model.rf2 <- randomForest(type ~ ., data=spam3[learn,], ntree=100, proximity=FALSE,
sampsize=c(nonspam=800, spam=500), strata=spam3[learn,]$type)
model.rf2
pred.rf2 <- predict (model.rf2, spam3[-learn,], type="class")
(ct <- table(Truth=spam3[-learn,]$type, Pred=pred.rf2))         # confusion matrix on test set
(prop.table(ct, 1))                                             # test error rates per class
(round(100*(1-sum(diag(ct))/sum(ct)),2))                        # test error rate
(F1 <- harm (prop.table(ct,1)[1,1], prop.table(ct,1)[2,2]))     # F1 measure on test set
(ntrees <- round(2^seq(1,10)))
rf.results <- matrix (rep(0,2*length(ntrees)), nrow=length(ntrees))
colnames (rf.results) <- c("ntrees", "OOB")
rf.results[,"ntrees"] <- ntrees
rf.results[,"OOB"] <- 0
ii <- 1
for (nt in ntrees)
{
print(nt)
# build forest
model.rf <- randomForest(type ~ ., data=spam3[learn,], ntree=nt, proximity=FALSE,
sampsize=c(nonspam=800, spam=500), strata=spam3[learn,]$type)
# get the OOB and store it appropriately
rf.results[ii, "OOB"] <- model.rf$err.rate[nt,1]
ii <- ii+1
}
rf.results
lowest.OOB.error <- as.integer(which.min(rf.results[,"OOB"]))
(ntrees.best <- rf.results[lowest.OOB.error,"ntrees"])
model.rf3 <- randomForest(type ~ ., data=spam3[learn,], ntree=ntrees.best, proximity=FALSE,
sampsize=c(nonspam=800, spam=500), strata=spam3[learn,]$type)
pred.rf3 <- predict (model.rf3, spam3[-learn,], type="class")
ct <- table(Truth=spam3[-learn,]$type, Pred=pred.rf3)
(prop.table(ct, 1))                             # test set error by class
(round(100*(1-sum(diag(ct))/sum(ct)),2))        # test set error
(F1 <- harm (prop.table(ct,1)[1,1], prop.table(ct,1)[2,2]))  # F1 measure
print(model.rf3)
summary(model.rf3)
model.rf3$oob.times
model.rf3$confusion
varUsed(model.rf3)
importance(model.rf3)
varImpPlot(model.rf3)
plot(model.rf3)
legend("topright", legend=c("OOB", "nonspam OOB", "spam OOB"), pch=c(1,1), col=c("black","red","green"))
# 1. build forest on the whole spam3 data
rf <- randomForest (type ~ ., data = spam3, ntree=1000, proximity=TRUE, importance=FALSE)
# 2. compute MDS based on pair-wise proximities from random forest and plot
rf.plot <- MDSplot(rf, spam3$type, pch=as.numeric(spam3$type))
# 3. add nice stuff to plot
library(RColorBrewer)
title("Data visualization based on MDS projection of the Random Forest")
legend("topright", legend=levels(spam3$type),
fill=brewer.pal(length(levels(spam3$type)), "Set1"))
# 4. need to identify some points? here, we include labels for first 100 emails..
text(rf.plot$points[1:100,], labels=attr(rf.plot$points,"dimnames")[[1]][1:10], cex=0.4)
setwd("C:/Users/bcoma/Documents/GitHub/Tesis_UB/scripts")
library(readxl)
#install.packages("ltm")
#install.packages("polycor")
#install.packages("glmmTMB")
library(polycor)
library("glmmTMB")
library("DescTools")
#cor <- correlation(allInfo)
#summary(cor)
allInfo <- read_excel('../output/allExcels_negatiu.xlsx')
allInfoLog <- allInfo[,]
#bx = boxcox(I(Budget_Previous_Year+1) ~ . - Visitado, data = allInfoLog,lambda = seq(-0.25, 0.25, length = 10))
allInfoLog['Budget_Previous_Year'] <- log2(allInfoLog['Budget_Previous_Year'])
allInfoLog['Donor_Aid_Budget'] <- log2(allInfoLog['Donor_Aid_Budget'])
allInfoLog['GDP'] <- log2(allInfoLog['GDP'])
allInfoLog['Public_Grant'] <- log2(allInfoLog['Public_Grant'])
allInfoLog[allInfoLog<0] <- 0
